{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbcbe65-0c3f-4635-af0c-3664fedb115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "from PIL import Image\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def compute_ndcg_at_k(relevance_scores: np.ndarray, k: int = 5) -> float:\n",
    "    \"\"\"Compute NDCG@K metric\"\"\"\n",
    "    if len(relevance_scores) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    k = min(k, len(relevance_scores))\n",
    "    top_k_scores = relevance_scores[:k]\n",
    "    \n",
    "    dcg = top_k_scores[0] + np.sum(top_k_scores[1:] / np.log2(np.arange(2, k + 1)))\n",
    "    \n",
    "    ideal_scores = np.sort(relevance_scores)[::-1][:k]\n",
    "    idcg = ideal_scores[0] + np.sum(ideal_scores[1:] / np.log2(np.arange(2, k + 1)))\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def compute_recall_at_k(ranked_indices: List[int], relevant_idx: int, k: int = 1) -> float:\n",
    "    return 1.0 if relevant_idx in ranked_indices[:k] else 0.0\n",
    "\n",
    "def compute_mrr(ranked_indices: List[int], relevant_idx: int) -> float:\n",
    "    try:\n",
    "        rank = ranked_indices.index(relevant_idx) + 1\n",
    "        return 1.0 / rank\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "class ArxivQADataset:\n",
    "    \"\"\"Wrapper for ArxivQA dataset from ViDoRe\"\"\"\n",
    "    \n",
    "    def __init__(self, split: str = \"test\"):\n",
    "        print(f\"Loading ArxivQA {split} split from HuggingFace...\")\n",
    "        self.dataset = load_dataset(\"vidore/arxivqa_test_subsampled\", split=split)\n",
    "        print(f\"Loaded {len(self.dataset)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "    \n",
    "    def get_queries_and_docs(self) -> Tuple[List[str], List[Image.Image], List[int]]:\n",
    "        \"\"\"Extract queries, document images, and relevance mappings\"\"\"\n",
    "        queries = []\n",
    "        doc_images = []\n",
    "        query_to_doc = []\n",
    "        \n",
    "        for item in self.dataset:\n",
    "            queries.append(item['query'])\n",
    "            doc_images.append(item['image'])\n",
    "            query_to_doc.append(len(doc_images) - 1)\n",
    "        \n",
    "        return queries, doc_images, query_to_doc\n",
    "\n",
    "class ColPaliRetriever:\n",
    "    \"\"\"ColPali model with late interaction mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"vidore/colpali-v1.2\"):\n",
    "        print(f\"Loading ColPali model: {model_name}\")\n",
    "        self.processor = ColPaliProcessor.from_pretrained(model_name)\n",
    "        self.model = ColPali.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.device = self.model.device\n",
    "        self.model.eval()\n",
    "        print(f\"Model loaded on device: {self.device}\")\n",
    "    \n",
    "    def encode_queries(self, queries: List[str], batch_size: int = 8) -> List[torch.Tensor]:\n",
    "        \"\"\"Encode text queries to multi-vector representations\"\"\"\n",
    "        all_query_embeddings = []\n",
    "        \n",
    "        print(f\"Encoding {len(queries)} queries...\")\n",
    "        for i in tqdm(range(0, len(queries), batch_size)):\n",
    "            batch = queries[i:i+batch_size]\n",
    "            \n",
    "            inputs = self.processor.process_queries(batch).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                query_embeddings = self.model(**inputs)\n",
    "            \n",
    "            all_query_embeddings.extend(list(query_embeddings))\n",
    "        \n",
    "        return all_query_embeddings\n",
    "    \n",
    "    def encode_images(self, images: List[Image.Image], batch_size: int = 4) -> List[torch.Tensor]:\n",
    "        \"\"\"Encode document images to multi-vector representations\"\"\"\n",
    "        all_doc_embeddings = []\n",
    "        \n",
    "        print(f\"Encoding {len(images)} document images...\")\n",
    "        for i in tqdm(range(0, len(images), batch_size)):\n",
    "            batch = images[i:i+batch_size]\n",
    "            \n",
    "            inputs = self.processor.process_images(batch).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                doc_embeddings = self.model(**inputs)\n",
    "            \n",
    "            all_doc_embeddings.extend(list(doc_embeddings))\n",
    "        \n",
    "        return all_doc_embeddings\n",
    "    \n",
    "    def late_interaction_score(self, query_embedding: torch.Tensor, \n",
    "                              doc_embedding: torch.Tensor) -> float:\n",
    "        \"\"\"Compute late interaction score between query and document\"\"\"\n",
    "\n",
    "        similarity_matrix = torch.matmul(query_embedding, doc_embedding.T)  \n",
    "        \n",
    "        max_scores = torch.max(similarity_matrix, dim=1)[0]  \n",
    "        \n",
    "        score = torch.sum(max_scores).item()\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def retrieve(self, query_embeddings: List[torch.Tensor], \n",
    "                doc_embeddings: List[torch.Tensor], \n",
    "                top_k: int = 10) -> List[List[int]]:\n",
    "        \"\"\"Retrieve top-k documents for each query using late interaction\"\"\"\n",
    "        rankings = []\n",
    "        \n",
    "        print(f\"Computing late interaction scores for {len(query_embeddings)} queries...\")\n",
    "        for query_emb in tqdm(query_embeddings):\n",
    "            scores = []\n",
    "            for doc_emb in doc_embeddings:\n",
    "                score = self.late_interaction_score(query_emb, doc_emb)\n",
    "                scores.append(score)\n",
    "            \n",
    "            scores = np.array(scores)\n",
    "            top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "            rankings.append(top_indices.tolist())\n",
    "        \n",
    "        return rankings\n",
    "\n",
    "def evaluate_retrieval(rankings: List[List[int]], \n",
    "                      relevant_docs: List[int]) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate retrieval performance\"\"\"\n",
    "    ndcg_5_scores = []\n",
    "    recall_1_scores = []\n",
    "    recall_5_scores = []\n",
    "    mrr_scores = []\n",
    "    \n",
    "    for ranking, relevant_idx in zip(rankings, relevant_docs):\n",
    "        relevance = np.array([1 if idx == relevant_idx else 0 for idx in ranking])\n",
    "        \n",
    "        ndcg_5_scores.append(compute_ndcg_at_k(relevance, k=5))\n",
    "        recall_1_scores.append(compute_recall_at_k(ranking, relevant_idx, k=1))\n",
    "        recall_5_scores.append(compute_recall_at_k(ranking, relevant_idx, k=5))\n",
    "        mrr_scores.append(compute_mrr(ranking, relevant_idx))\n",
    "    \n",
    "    return {\n",
    "        \"NDCG@5\": np.mean(ndcg_5_scores) * 100,\n",
    "        \"Recall@1\": np.mean(recall_1_scores) * 100,\n",
    "        \"Recall@5\": np.mean(recall_5_scores) * 100,\n",
    "        \"MRR\": np.mean(mrr_scores)\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*80)\n",
    "    print(\"ColPali Evaluation on ArxivQA Dataset\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    dataset = ArxivQADataset(split=\"test\")\n",
    "    queries, doc_images, query_to_doc = dataset.get_queries_and_docs()\n",
    "    \n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Number of queries: {len(queries)}\")\n",
    "    print(f\"  Number of documents: {len(doc_images)}\")\n",
    "    print(f\"  Sample query: {queries[0][:100]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Evaluating ColPali\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    colpali = ColPaliRetriever(\"vidore/colpali-v1.2\")\n",
    "    \n",
    "    query_embeddings = colpali.encode_queries(queries, batch_size=8)\n",
    "    \n",
    "    doc_embeddings = colpali.encode_images(doc_images, batch_size=4)\n",
    "    \n",
    "    rankings = colpali.retrieve(query_embeddings, doc_embeddings, top_k=10)\n",
    "    \n",
    "    results = evaluate_retrieval(rankings, query_to_doc)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COLPALI RESULTS ON ARXIVQA DATASET\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nNDCG@5:   {results['NDCG@5']:.2f}\")\n",
    "    print(f\"Recall@1: {results['Recall@1']:.2f}\")\n",
    "    print(f\"Recall@5: {results['Recall@5']:.2f}\")\n",
    "    print(f\"MRR:      {results['MRR']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Expected results from paper (Table 2):\")\n",
    "    print(\"  NDCG@5:   79.1\")\n",
    "    print(\"  Recall@1: 72.4\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e13d3-a26f-42ec-8769-03b0805351bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch colpali-engine datasets pillow numpy tqdm\n",
    "\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1c11e-05e5-453b-a526-0ce0d137f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Two-Stage RAG System for ArxivQA Dataset\n",
    "Stage 1: Retrieve relevant document\n",
    "Stage 2: Retrieve relevant page from document\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "from PIL import Image\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def compute_ndcg_at_k(relevance_scores: np.ndarray, k: int = 5) -> float:\n",
    "    \"\"\"Compute NDCG@K metric\"\"\"\n",
    "    if len(relevance_scores) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    k = min(k, len(relevance_scores))\n",
    "    top_k_scores = relevance_scores[:k]\n",
    "    \n",
    "    dcg = top_k_scores[0] + np.sum(top_k_scores[1:] / np.log2(np.arange(2, k + 1)))\n",
    "    \n",
    "    ideal_scores = np.sort(relevance_scores)[::-1][:k]\n",
    "    idcg = ideal_scores[0] + np.sum(ideal_scores[1:] / np.log2(np.arange(2, k + 1)))\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def compute_recall_at_k(ranked_indices: List[int], relevant_idx: int, k: int = 1) -> float:\n",
    "    \"\"\"Compute Recall@K metric\"\"\"\n",
    "    return 1.0 if relevant_idx in ranked_indices[:k] else 0.0\n",
    "\n",
    "def compute_mrr(ranked_indices: List[int], relevant_idx: int) -> float:\n",
    "    \"\"\"Compute Mean Reciprocal Rank\"\"\"\n",
    "    try:\n",
    "        rank = ranked_indices.index(relevant_idx) + 1\n",
    "        return 1.0 / rank\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "class ArxivQADatasetWithDocs:\n",
    "    \"\"\"ArxivQA dataset organized by documents\"\"\"\n",
    "    \n",
    "    def __init__(self, split: str = \"test\"):\n",
    "        print(f\"Loading ArxivQA {split} split from HuggingFace...\")\n",
    "        self.dataset = load_dataset(\"vidore/arxivqa_test_subsampled\", split=split)\n",
    "        print(f\"Loaded {len(self.dataset)} samples\")\n",
    "        \n",
    "        self._organize_by_documents()\n",
    "    \n",
    "    def _organize_by_documents(self):\n",
    "        \"\"\"Group pages by their parent document\"\"\"\n",
    "        print(\"Organizing pages by documents...\")\n",
    "        \n",
    "        self.doc_to_pages = defaultdict(list)\n",
    "        \n",
    "        self.query_to_doc_page = {}\n",
    "        \n",
    "        self.documents = []\n",
    "        \n",
    "        PAGES_PER_DOC = 5  \n",
    "        \n",
    "        for idx, item in enumerate(self.dataset):\n",
    "            doc_id = idx // PAGES_PER_DOC  \n",
    "            page_in_doc = idx % PAGES_PER_DOC\n",
    "            \n",
    "            self.doc_to_pages[doc_id].append((idx, item['image']))\n",
    "            \n",
    "            self.query_to_doc_page[idx] = (doc_id, page_in_doc)\n",
    "        \n",
    "        self.document_ids = sorted(self.doc_to_pages.keys())\n",
    "        \n",
    "        print(f\"Organized into {len(self.document_ids)} documents\")\n",
    "        print(f\"Average pages per document: {len(self.dataset) / len(self.document_ids):.1f}\")\n",
    "    \n",
    "    def get_queries(self) -> List[str]:\n",
    "        \"\"\"Get all queries\"\"\"\n",
    "        return [item['query'] for item in self.dataset]\n",
    "    \n",
    "    def get_document_representations(self) -> Tuple[List[int], List[List[Image.Image]]]:\n",
    "        \"\"\"Get document IDs and their pages for indexing\"\"\"\n",
    "        doc_ids = []\n",
    "        doc_pages = []\n",
    "        \n",
    "        for doc_id in self.document_ids:\n",
    "            doc_ids.append(doc_id)\n",
    "            pages = [img for _, img in self.doc_to_pages[doc_id]]\n",
    "            doc_pages.append(pages)\n",
    "        \n",
    "        return doc_ids, doc_pages\n",
    "    \n",
    "    def get_all_pages(self) -> List[Image.Image]:\n",
    "        \"\"\"Get all individual pages\"\"\"\n",
    "        return [item['image'] for item in self.dataset]\n",
    "    \n",
    "    def get_ground_truth(self, query_idx: int) -> Tuple[int, int]:\n",
    "        \"\"\"Get ground truth (doc_id, page_idx_in_doc) for a query\"\"\"\n",
    "        return self.query_to_doc_page[query_idx]\n",
    "    \n",
    "    def get_page_global_idx(self, doc_id: int, page_in_doc: int) -> int:\n",
    "        \"\"\"Convert (doc_id, page_in_doc) to global page index\"\"\"\n",
    "        return self.doc_to_pages[doc_id][page_in_doc][0]\n",
    "\n",
    "class TwoStageColPaliRAG:\n",
    "    \"\"\"Two-stage retrieval system using ColPali\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"vidore/colpali-v1.2\"):\n",
    "        print(f\"Loading ColPali model: {model_name}\")\n",
    "        self.processor = ColPaliProcessor.from_pretrained(model_name)\n",
    "        self.model = ColPali.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.device = self.model.device\n",
    "        self.model.eval()\n",
    "        print(f\"Model loaded on device: {self.device}\")\n",
    "        \n",
    "        self.doc_embeddings = None  \n",
    "        self.page_embeddings = None  \n",
    "        self.doc_ids = None\n",
    "        self.doc_to_pages_map = None\n",
    "    \n",
    "    def encode_queries(self, queries: List[str], batch_size: int = 8) -> List[torch.Tensor]:\n",
    "        \"\"\"Encode text queries\"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        print(f\"Encoding {len(queries)} queries...\")\n",
    "        for i in tqdm(range(0, len(queries), batch_size)):\n",
    "            batch = queries[i:i+batch_size]\n",
    "            inputs = self.processor.process_queries(batch).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embeddings = self.model(**inputs)\n",
    "            \n",
    "            all_embeddings.extend(list(embeddings))\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    def encode_images(self, images: List[Image.Image], batch_size: int = 4) -> List[torch.Tensor]:\n",
    "        \"\"\"Encode images\"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(images), batch_size)):\n",
    "            batch = images[i:i+batch_size]\n",
    "            inputs = self.processor.process_images(batch).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embeddings = self.model(**inputs)\n",
    "            \n",
    "            all_embeddings.extend(list(embeddings))\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    def late_interaction_score(self, query_emb: torch.Tensor, doc_emb: torch.Tensor) -> float:\n",
    "        \"\"\"Compute late interaction score\"\"\"\n",
    "        similarity_matrix = torch.matmul(query_emb, doc_emb.T)\n",
    "        max_scores = torch.max(similarity_matrix, dim=1)[0]\n",
    "        return torch.sum(max_scores).item()\n",
    "    \n",
    "    def index_documents(self, doc_ids: List[int], doc_pages: List[List[Image.Image]], \n",
    "                       batch_size: int = 4):\n",
    "        \"\"\"\n",
    "        Stage 1 Indexing: Create document-level representations\n",
    "        by aggregating/averaging page embeddings per document\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STAGE 1: Indexing Documents\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        self.doc_ids = doc_ids\n",
    "        self.doc_embeddings = []\n",
    "        self.doc_to_pages_map = {}\n",
    "        \n",
    "        for doc_id, pages in tqdm(zip(doc_ids, doc_pages), total=len(doc_ids), \n",
    "                                   desc=\"Encoding documents\"):\n",
    "            page_embeddings = self.encode_images(pages, batch_size=batch_size)\n",
    "            \n",
    "            stacked_pages = torch.stack(page_embeddings)  \n",
    "            doc_embedding = torch.mean(stacked_pages, dim=0)  \n",
    "            \n",
    "            self.doc_embeddings.append(doc_embedding)\n",
    "            self.doc_to_pages_map[doc_id] = page_embeddings\n",
    "        \n",
    "        print(f\"Indexed {len(self.doc_embeddings)} documents\")\n",
    "    \n",
    "    def index_all_pages(self, all_pages: List[Image.Image], batch_size: int = 4):\n",
    "        \"\"\"\n",
    "        Stage 2 Indexing: Index all individual pages\n",
    "        (Alternative: only index pages from retrieved documents)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STAGE 2: Indexing All Pages\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(\"Encoding all pages...\")\n",
    "        self.page_embeddings = self.encode_images(all_pages, batch_size=batch_size)\n",
    "        print(f\"Indexed {len(self.page_embeddings)} pages\")\n",
    "    \n",
    "    def retrieve_document(self, query_embedding: torch.Tensor, top_k: int = 5) -> List[int]:\n",
    "        \"\"\"\n",
    "        Stage 1: Retrieve top-k documents for the query\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for doc_emb in self.doc_embeddings:\n",
    "            score = self.late_interaction_score(query_embedding, doc_emb)\n",
    "            scores.append(score)\n",
    "        \n",
    "        scores = np.array(scores)\n",
    "        top_doc_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        \n",
    "        return [self.doc_ids[idx] for idx in top_doc_indices]\n",
    "    \n",
    "    def retrieve_page_from_document(self, query_embedding: torch.Tensor, \n",
    "                                    doc_id: int, top_k: int = 5) -> List[int]:\n",
    "        \"\"\"\n",
    "        Stage 2: Retrieve top-k pages from a specific document\n",
    "        \"\"\"\n",
    "        page_embeddings = self.doc_to_pages_map[doc_id]\n",
    "        \n",
    "        scores = []\n",
    "        for page_emb in page_embeddings:\n",
    "            score = self.late_interaction_score(query_embedding, page_emb)\n",
    "            scores.append(score)\n",
    "        \n",
    "        scores = np.array(scores)\n",
    "        top_page_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        \n",
    "        return top_page_indices.tolist()\n",
    "    \n",
    "    def retrieve_two_stage(self, query_embeddings: List[torch.Tensor],\n",
    "                          top_k_docs: int = 3, top_k_pages: int = 5) -> List[List[Tuple[int, int]]]:\n",
    "        \"\"\"\n",
    "        Two-stage retrieval:\n",
    "        1. Retrieve top-k documents\n",
    "        2. For each document, retrieve top-k pages\n",
    "        Returns list of (doc_id, page_in_doc) tuples for each query\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TWO-STAGE RETRIEVAL\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for query_emb in tqdm(query_embeddings, desc=\"Retrieving\"):\n",
    "            top_docs = self.retrieve_document(query_emb, top_k=top_k_docs)\n",
    "            \n",
    "            doc_page_scores = []\n",
    "            \n",
    "            for doc_id in top_docs:\n",
    "                top_pages = self.retrieve_page_from_document(query_emb, doc_id, top_k=top_k_pages)\n",
    "                \n",
    "                for page_idx in top_pages:\n",
    "                    page_emb = self.doc_to_pages_map[doc_id][page_idx]\n",
    "                    score = self.late_interaction_score(query_emb, page_emb)\n",
    "                    doc_page_scores.append(((doc_id, page_idx), score))\n",
    "            \n",
    "            doc_page_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_results = [dp for dp, _ in doc_page_scores[:top_k_pages]]\n",
    "            \n",
    "            all_results.append(top_results)\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "def evaluate_two_stage(results: List[List[Tuple[int, int]]], \n",
    "                      ground_truth: List[Tuple[int, int]],\n",
    "                      dataset: ArxivQADatasetWithDocs) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate two-stage retrieval results\"\"\"\n",
    "    \n",
    "    ndcg_5_scores = []\n",
    "    recall_1_scores = []\n",
    "    recall_5_scores = []\n",
    "    mrr_scores = []\n",
    "    \n",
    "    doc_recall_scores = []\n",
    "    page_given_doc_recall_scores = []\n",
    "    \n",
    "    for result_list, (gt_doc_id, gt_page_in_doc) in zip(results, ground_truth):\n",
    "\n",
    "        retrieved_global_indices = []\n",
    "        for doc_id, page_in_doc in result_list:\n",
    "            global_idx = dataset.get_page_global_idx(doc_id, page_in_doc)\n",
    "            retrieved_global_indices.append(global_idx)\n",
    "        \n",
    "        gt_global_idx = dataset.get_page_global_idx(gt_doc_id, gt_page_in_doc)\n",
    "        \n",
    "        relevance = np.array([1 if idx == gt_global_idx else 0 for idx in retrieved_global_indices])\n",
    "        \n",
    "        ndcg_5_scores.append(compute_ndcg_at_k(relevance, k=5))\n",
    "        recall_1_scores.append(compute_recall_at_k(retrieved_global_indices, gt_global_idx, k=1))\n",
    "        recall_5_scores.append(compute_recall_at_k(retrieved_global_indices, gt_global_idx, k=5))\n",
    "        mrr_scores.append(compute_mrr(retrieved_global_indices, gt_global_idx))\n",
    "        \n",
    "        retrieved_docs = [doc_id for doc_id, _ in result_list]\n",
    "        doc_correct = 1.0 if gt_doc_id in retrieved_docs else 0.0\n",
    "        doc_recall_scores.append(doc_correct)\n",
    "\n",
    "        if doc_correct:\n",
    "            pages_from_correct_doc = [page for doc, page in result_list if doc == gt_doc_id]\n",
    "            page_correct = 1.0 if gt_page_in_doc in pages_from_correct_doc else 0.0\n",
    "            page_given_doc_recall_scores.append(page_correct)\n",
    "    \n",
    "    return {\n",
    "        \"NDCG@5\": np.mean(ndcg_5_scores) * 100,\n",
    "        \"Recall@1\": np.mean(recall_1_scores) * 100,\n",
    "        \"Recall@5\": np.mean(recall_5_scores) * 100,\n",
    "        \"MRR\": np.mean(mrr_scores),\n",
    "        \"Doc_Recall\": np.mean(doc_recall_scores) * 100,\n",
    "        \"Page_Given_Doc_Recall\": np.mean(page_given_doc_recall_scores) * 100 if page_given_doc_recall_scores else 0.0,\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*80)\n",
    "    print(\"TWO-STAGE RAG SYSTEM - ArxivQA Evaluation\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    dataset = ArxivQADatasetWithDocs(split=\"test\")\n",
    "    \n",
    "    queries = dataset.get_queries()\n",
    "    doc_ids, doc_pages = dataset.get_document_representations()\n",
    "    all_pages = dataset.get_all_pages()\n",
    "    \n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Total queries: {len(queries)}\")\n",
    "    print(f\"  Total documents: {len(doc_ids)}\")\n",
    "    print(f\"  Total pages: {len(all_pages)}\")\n",
    "\n",
    "    rag = TwoStageColPaliRAG(\"vidore/colpali-v1.2\")\n",
    "\n",
    "    rag.index_documents(doc_ids, doc_pages, batch_size=4)\n",
    "\n",
    "    print(\"\\nEncoding queries...\")\n",
    "    query_embeddings = rag.encode_queries(queries, batch_size=8)\n",
    "\n",
    "    results = rag.retrieve_two_stage(query_embeddings, top_k_docs=3, top_k_pages=5)\n",
    "\n",
    "    ground_truth = [dataset.get_ground_truth(i) for i in range(len(queries))]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    metrics = evaluate_two_stage(results, ground_truth, dataset)\n",
    "    \n",
    "    print(\"\\nStandard Retrieval Metrics:\")\n",
    "    print(f\"  NDCG@5:   {metrics['NDCG@5']:.2f}\")\n",
    "    print(f\"  Recall@1: {metrics['Recall@1']:.2f}\")\n",
    "    print(f\"  Recall@5: {metrics['Recall@5']:.2f}\")\n",
    "    print(f\"  MRR:      {metrics['MRR']:.4f}\")\n",
    "    \n",
    "    print(\"\\nTwo-Stage Specific Metrics:\")\n",
    "    print(f\"  Document Recall:           {metrics['Doc_Recall']:.2f}%\")\n",
    "    print(f\"  Page Recall (given doc):   {metrics['Page_Given_Doc_Recall']:.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Comparison to Single-Stage ColPali (from paper):\")\n",
    "    print(\"  Expected NDCG@5:   79.1\")\n",
    "    print(\"  Expected Recall@1: 72.4\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11b2f9-6add-4671-b0ad-780a91a3adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "class OpenRAGBenchLoader:\n",
    "    \"\"\"\n",
    "    Load and process vectara/open_ragbench dataset for evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_dir=\"./open_ragbench\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_dir: Where to store downloaded dataset\n",
    "        \"\"\"\n",
    "        self.dataset_dir = Path(dataset_dir)\n",
    "        self.dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.base_url = \"https://huggingface.co/datasets/vectara/open_ragbench/raw/main\"\n",
    "        \n",
    "        self.queries = None\n",
    "        self.answers = None\n",
    "        self.corpus = {}\n",
    "        self.qrels = None\n",
    "        self.pdf_urls = None\n",
    "    \n",
    "\n",
    "    def download_dataset(self, max_queries=None):\n",
    "        \"\"\"\n",
    "        Download the Open RAGBench dataset (metadata + PDFs)\n",
    "\n",
    "        Args:\n",
    "            max_queries: number of queries to process (downloads PDFs for these queries)\n",
    "        \"\"\"\n",
    "        \n",
    "        files_to_download = {\n",
    "            'queries.json': f\"{self.base_url}/pdf/arxiv/queries.json\",\n",
    "            'answers.json': f\"{self.base_url}/pdf/arxiv/answers.json\",\n",
    "            'pdf_urls.json': f\"{self.base_url}/pdf/arxiv/pdf_urls.json\",\n",
    "            'qrels.json': f\"{self.base_url}/pdf/arxiv/qrels.json\",\n",
    "        }\n",
    "        \n",
    "        print(\"Downloading Open RAGBench dataset...\")\n",
    "\n",
    "        for filename, url in files_to_download.items():\n",
    "            filepath = self.dataset_dir / filename\n",
    "            \n",
    "            if filepath.exists():\n",
    "                print(f\"  ✓ {filename} already exists\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Downloading {filename}...\")\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open(filepath, 'w') as f:\n",
    "                    f.write(response.text)\n",
    "            else:\n",
    "                print(f\"  ✗ Failed to download {filename}: {response.status_code}\")\n",
    "        \n",
    "        print(\"\\n  Downloading PDFs...\")\n",
    "        self._download_pdfs_for_queries(max_queries=max_queries)\n",
    "        \n",
    "        print(\"\\n✓ Dataset download complete!\")\n",
    "    \n",
    "    def _download_pdfs_for_queries(self, max_queries=None, output_dir=\"./arxiv_pdfs\"):\n",
    "        \"\"\"\n",
    "        Download PDFs required for the first N queries.\n",
    "        \n",
    "        Args:\n",
    "            max_queries: Number of queries to process\n",
    "            output_dir: Where to save PDFs\n",
    "        \"\"\"\n",
    "        pdf_urls_path = self.dataset_dir / \"pdf_urls.json\"\n",
    "        qrels_path = self.dataset_dir / \"qrels.json\"\n",
    "        \n",
    "        if not pdf_urls_path.exists() or not qrels_path.exists():\n",
    "            print(\"  ✗ Missing pdf_urls.json or qrels.json\")\n",
    "            return\n",
    "        \n",
    "        with open(pdf_urls_path, 'r') as f:\n",
    "            pdf_urls = json.load(f)\n",
    "        \n",
    "        with open(qrels_path, 'r') as f:\n",
    "            qrels = json.load(f)\n",
    "        \n",
    "        query_ids = list(qrels.keys())\n",
    "        if max_queries is not None:\n",
    "            query_ids = query_ids[:max_queries]\n",
    "        \n",
    "        arxiv_ids = set()\n",
    "        for query_id in query_ids:\n",
    "            docs = qrels[query_id]\n",
    "            if isinstance(docs, dict) and 'doc_id' in docs:\n",
    "                arxiv_ids.add(docs['doc_id'])\n",
    "            elif isinstance(docs, dict):\n",
    "                for doc_id in docs.keys():\n",
    "                    if '_page_' in doc_id:\n",
    "                        arxiv_id = doc_id.split('_page_')[0]\n",
    "                        arxiv_ids.add(arxiv_id)\n",
    "        \n",
    "        arxiv_ids = list(arxiv_ids)\n",
    "        \n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"  Downloading {len(arxiv_ids)} PDFs for {len(query_ids)} queries...\")\n",
    "        \n",
    "        for arxiv_id in tqdm(arxiv_ids, desc=\"  Downloading PDFs\"):\n",
    "            pdf_path = output_path / f\"{arxiv_id}.pdf\"\n",
    "            \n",
    "            if pdf_path.exists():\n",
    "                continue\n",
    "            \n",
    "            if arxiv_id not in pdf_urls:\n",
    "                print(f\"    ✗ No URL found for {arxiv_id}\")\n",
    "                continue\n",
    "            \n",
    "            url = pdf_urls[arxiv_id]\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url, timeout=20)\n",
    "                if response.status_code == 200:\n",
    "                    with open(pdf_path, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                else:\n",
    "                    print(f\"    ✗ Failed for {arxiv_id}: HTTP {response.status_code}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ✗ Error downloading {arxiv_id}: {e}\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "        \n",
    "        print(f\"  ✓ PDFs stored in {output_dir}/\")\n",
    "    \n",
    "\n",
    "    def load_dataset(self):\n",
    "        \"\"\"Load downloaded dataset into memory.\"\"\"\n",
    "        \n",
    "        print(\"Loading Open RAGBench dataset...\")\n",
    "        \n",
    "\n",
    "        queries_path = self.dataset_dir / \"queries.json\"\n",
    "        if queries_path.exists():\n",
    "            with open(queries_path, 'r') as f:\n",
    "                self.queries = json.load(f)\n",
    "            print(f\"  ✓ Loaded {len(self.queries)} queries\")\n",
    "        \n",
    "\n",
    "        answers_path = self.dataset_dir / \"answers.json\"\n",
    "        if answers_path.exists():\n",
    "            with open(answers_path, 'r') as f:\n",
    "                self.answers = json.load(f)\n",
    "            print(f\"  ✓ Loaded {len(self.answers)} answers\")\n",
    "        \n",
    "\n",
    "        qrels_path = self.dataset_dir / \"qrels.json\"\n",
    "        if qrels_path.exists():\n",
    "            with open(qrels_path, 'r') as f:\n",
    "                self.qrels = json.load(f)\n",
    "            print(f\"  ✓ Loaded relevance judgments for {len(self.qrels)} queries\")\n",
    "        \n",
    "\n",
    "        corpus_dir = self.dataset_dir / \"corpus\"\n",
    "        if corpus_dir.exists():\n",
    "            for corpus_file in corpus_dir.glob(\"*.json\"):\n",
    "                with open(corpus_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    self.corpus.update(data)\n",
    "            print(f\"  ✓ Loaded {len(self.corpus)} corpus documents\")\n",
    "        \n",
    "        print(\"✓ Dataset loaded!\\n\")\n",
    "    \n",
    "    def get_ground_truth_pages(self, query_id):\n",
    "        \"\"\"Extract (arxiv_id, page_num) pairs from qrels.\"\"\"\n",
    "        \n",
    "        if query_id not in self.qrels:\n",
    "            return []\n",
    "        \n",
    "        ground_truth = []\n",
    "        docs = self.qrels[query_id]\n",
    "        \n",
    "        if isinstance(docs, dict) and 'doc_id' in docs:\n",
    "\n",
    "            arxiv_id = docs['doc_id']\n",
    "            section_id = docs['section_id']\n",
    "            ground_truth.append((arxiv_id, section_id))\n",
    "        elif isinstance(docs, dict):\n",
    "\n",
    "            for doc_id in docs.keys():\n",
    "                if '_page_' in doc_id:\n",
    "                    arxiv_id, page = doc_id.split('_page_')\n",
    "                    ground_truth.append((arxiv_id, int(page)))\n",
    "        \n",
    "        return ground_truth\n",
    "    \n",
    "    def convert_to_evaluation_format(self, max_samples=None):\n",
    "        \"\"\"Convert dataset into format used by RAGEvaluator.\"\"\"\n",
    "        \n",
    "        if not self.queries or not self.answers:\n",
    "            raise ValueError(\"Dataset not loaded. Call load_dataset() first.\")\n",
    "        \n",
    "        evaluation_dataset = []\n",
    "        query_ids = list(self.queries.keys())\n",
    "        \n",
    "        if max_samples:\n",
    "            query_ids = query_ids[:max_samples]\n",
    "        \n",
    "        for query_id in query_ids:\n",
    "            query_data = self.queries[query_id]\n",
    "            ground_truth = self.get_ground_truth_pages(query_id)\n",
    "            \n",
    "            evidence_pages = [\n",
    "                f\"{arxiv_id}_page_{page_num:03d}\"\n",
    "                for arxiv_id, page_num in ground_truth\n",
    "            ]\n",
    "            \n",
    "            evaluation_dataset.append({\n",
    "                \"query_id\": query_id,\n",
    "                \"question\": query_data[\"query\"],\n",
    "                \"answer\": self.answers.get(query_id, \"\"),\n",
    "                \"evidence_pages\": evidence_pages,\n",
    "                \"evidence_type\": query_data.get(\"source\", \"unknown\"),\n",
    "                \"query_type\": query_data.get(\"type\", \"unknown\"),\n",
    "                \"page_count\": \"multi\" if len(evidence_pages) > 1 else \"single\"\n",
    "            })\n",
    "        \n",
    "        print(f\"✓ Converted {len(evaluation_dataset)} queries to evaluation format\")\n",
    "        return evaluation_dataset\n",
    "\n",
    "    def download_pdfs(self, output_dir=\"./arxiv_pdfs\", max_pdfs=None):\n",
    "        \"\"\"\n",
    "        Download PDF files using pdf_urls.json (standalone method).\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Where to save PDFs\n",
    "            max_pdfs: Maximum number of PDFs to download\n",
    "        \"\"\"\n",
    "        \n",
    "        pdf_urls_path = self.dataset_dir / \"pdf_urls.json\"\n",
    "        \n",
    "        if not pdf_urls_path.exists():\n",
    "            print(\"✗ pdf_urls.json not found. Run download_dataset() first.\")\n",
    "            return\n",
    "        \n",
    "        with open(pdf_urls_path, 'r') as f:\n",
    "            pdf_urls = json.load(f)\n",
    "        \n",
    "        arxiv_ids = list(pdf_urls.keys())\n",
    "        \n",
    "        if max_pdfs is not None:\n",
    "            arxiv_ids = arxiv_ids[:max_pdfs]\n",
    "        \n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"Downloading {len(arxiv_ids)} PDFs...\")\n",
    "        \n",
    "        for arxiv_id in tqdm(arxiv_ids, desc=\"Downloading PDFs\"):\n",
    "            pdf_path = output_path / f\"{arxiv_id}.pdf\"\n",
    "            \n",
    "            if pdf_path.exists():\n",
    "                continue\n",
    "            \n",
    "            url = pdf_urls[arxiv_id]\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url, timeout=20)\n",
    "                if response.status_code == 200:\n",
    "                    with open(pdf_path, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                else:\n",
    "                    print(f\"  ✗ Failed for {arxiv_id}: HTTP {response.status_code}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error downloading {arxiv_id}: {e}\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "        \n",
    "        print(f\"✓ PDFs stored in {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18405b9-2acd-4e4f-9a85-69e67de3438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Two-Stage RAG System for Open RAGBench Dataset\n",
    "Stage 1: Retrieve relevant PDF documents\n",
    "Stage 2: Retrieve relevant pages from documents\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "from PIL import Image\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import json\n",
    "from pathlib import Path\n",
    "import fitz\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fitz.TOOLS.mupdf_display_errors(False)\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "\n",
    "def compute_ndcg_at_k(relevance_scores: np.ndarray, k: int = 5) -> float:\n",
    "    if len(relevance_scores) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    k = min(k, len(relevance_scores))\n",
    "    top_k_scores = relevance_scores[:k]\n",
    "    \n",
    "    dcg = top_k_scores[0] + np.sum(top_k_scores[1:] / np.log2(np.arange(2, k + 1)))\n",
    "    \n",
    "    ideal_scores = np.sort(relevance_scores)[::-1][:k]\n",
    "    idcg = ideal_scores[0] + np.sum(ideal_scores[1:] / np.log2(np.arange(2, k + 1)))\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def compute_recall_at_k(ranked_indices: List[int], relevant_idx: int, k: int = 1) -> float:\n",
    "    return 1.0 if relevant_idx in ranked_indices[:k] else 0.0\n",
    "\n",
    "def compute_mrr(ranked_indices: List[int], relevant_idx: int) -> float:\n",
    "    try:\n",
    "        rank = ranked_indices.index(relevant_idx) + 1\n",
    "        return 1.0 / rank\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "class PDFProcessor:\n",
    "    \n",
    "    def __init__(self, pdf_dir: str = \"./arxiv_pdfs\", dpi: int = 144):\n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.dpi = dpi\n",
    "        self.pdf_cache = {}\n",
    "    \n",
    "    def pdf_to_images(self, arxiv_id: str) -> List[Image.Image]:\n",
    "        if arxiv_id in self.pdf_cache:\n",
    "            return self.pdf_cache[arxiv_id]\n",
    "        \n",
    "        pdf_path = self.pdf_dir / f\"{arxiv_id}.pdf\"\n",
    "        \n",
    "        if not pdf_path.exists():\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            images = []\n",
    "            \n",
    "            for page_num in range(len(doc)):\n",
    "                try:\n",
    "                    page = doc[page_num]\n",
    "                    pix = page.get_pixmap(dpi=self.dpi)\n",
    "                    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "                    images.append(img)\n",
    "                except Exception as page_error:\n",
    "                    print(f\"  Warning: Skipping page {page_num} of {arxiv_id}: {page_error}\")\n",
    "                    continue\n",
    "            \n",
    "            doc.close()\n",
    "            \n",
    "            if len(images) > 0:\n",
    "                self.pdf_cache[arxiv_id] = images\n",
    "            \n",
    "            return images\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing PDF {arxiv_id}: {str(e)[:100]}\")\n",
    "            return []\n",
    "\n",
    "class OpenRAGBenchDataset:\n",
    "    \n",
    "    def __init__(self, loader: OpenRAGBenchLoader, pdf_processor: PDFProcessor, max_queries: int = None):\n",
    "        self.loader = loader\n",
    "        self.pdf_processor = pdf_processor\n",
    "        self.max_queries = max_queries\n",
    "        \n",
    "        self._organize_data()\n",
    "    \n",
    "    def _organize_data(self):\n",
    "        print(\"Organizing Open RAGBench dataset...\")\n",
    "        \n",
    "        self.queries = []\n",
    "        self.query_ids = []\n",
    "        self.ground_truth = []\n",
    "        \n",
    "        all_query_ids = list(self.loader.queries.keys())\n",
    "        \n",
    "        if self.max_queries is not None:\n",
    "            all_query_ids = all_query_ids[:self.max_queries]\n",
    "            print(f\"Limiting to first {self.max_queries} queries\")\n",
    "        \n",
    "        for query_id in all_query_ids:\n",
    "            query_data = self.loader.queries[query_id]\n",
    "            self.query_ids.append(query_id)\n",
    "            self.queries.append(query_data['query'])\n",
    "            \n",
    "            gt_pages = self.loader.get_ground_truth_pages(query_id)\n",
    "            if gt_pages:\n",
    "                self.ground_truth.append(gt_pages[0])\n",
    "            else:\n",
    "                self.ground_truth.append((None, None))\n",
    "        \n",
    "        print(f\"Processing {len(self.queries)} queries\")\n",
    "        \n",
    "        arxiv_ids = set()\n",
    "        for gt in self.ground_truth:\n",
    "            if gt[0] is not None:\n",
    "                arxiv_ids.add(gt[0])\n",
    "        \n",
    "        available_pdfs = set()\n",
    "        pdf_dir = Path(self.pdf_processor.pdf_dir)\n",
    "        for pdf_file in pdf_dir.glob(\"*.pdf\"):\n",
    "            arxiv_id = pdf_file.stem\n",
    "            available_pdfs.add(arxiv_id)\n",
    "        \n",
    "        print(f\"Found {len(available_pdfs)} available PDFs\")\n",
    "        print(f\"Need {len(arxiv_ids)} documents for these queries\")\n",
    "        \n",
    "        arxiv_ids = arxiv_ids.intersection(available_pdfs)\n",
    "        self.arxiv_ids = sorted(list(arxiv_ids))\n",
    "        \n",
    "        filtered_queries = []\n",
    "        filtered_query_ids = []\n",
    "        filtered_ground_truth = []\n",
    "        \n",
    "        for i, (qid, query, gt) in enumerate(zip(self.query_ids, self.queries, self.ground_truth)):\n",
    "            if gt[0] in arxiv_ids:\n",
    "                filtered_queries.append(query)\n",
    "                filtered_query_ids.append(qid)\n",
    "                filtered_ground_truth.append(gt)\n",
    "        \n",
    "        self.queries = filtered_queries\n",
    "        self.query_ids = filtered_query_ids\n",
    "        self.ground_truth = filtered_ground_truth\n",
    "        \n",
    "        print(f\"✓ Filtered to {len(self.queries)} queries with available documents\")\n",
    "        \n",
    "        print(f\"Loading pages from {len(self.arxiv_ids)} documents...\")\n",
    "        self.doc_to_pages = {}\n",
    "        failed_pdfs = []\n",
    "        \n",
    "        for arxiv_id in tqdm(self.arxiv_ids, desc=\"Converting PDFs to images\"):\n",
    "            images = self.pdf_processor.pdf_to_images(arxiv_id)\n",
    "            if len(images) > 0:\n",
    "                self.doc_to_pages[arxiv_id] = images\n",
    "            else:\n",
    "                failed_pdfs.append(arxiv_id)\n",
    "        \n",
    "        if failed_pdfs:\n",
    "            print(f\"⚠ Warning: {len(failed_pdfs)} PDFs failed to load\")\n",
    "            filtered_queries = []\n",
    "            filtered_query_ids = []\n",
    "            filtered_ground_truth = []\n",
    "            \n",
    "            for i, (qid, query, gt) in enumerate(zip(self.query_ids, self.queries, self.ground_truth)):\n",
    "                if gt[0] not in failed_pdfs:\n",
    "                    filtered_queries.append(query)\n",
    "                    filtered_query_ids.append(qid)\n",
    "                    filtered_ground_truth.append(gt)\n",
    "            \n",
    "            self.queries = filtered_queries\n",
    "            self.query_ids = filtered_query_ids\n",
    "            self.ground_truth = filtered_ground_truth\n",
    "        \n",
    "        print(f\"✓ Successfully loaded {len(self.queries)} queries\")\n",
    "        print(f\"✓ Successfully loaded {len(self.doc_to_pages)} documents\")\n",
    "        total_pages = sum(len(pages) for pages in self.doc_to_pages.values())\n",
    "        print(f\"✓ Total pages: {total_pages}\")\n",
    "        avg_pages = total_pages / len(self.doc_to_pages) if self.doc_to_pages else 0\n",
    "        print(f\"✓ Average pages per document: {avg_pages:.1f}\")\n",
    "\n",
    "class TwoStageColPaliRAG:\n",
    "    \n",
    "    def __init__(self, model_name: str = \"vidore/colpali-v1.2\"):\n",
    "        print(f\"Loading ColPali model: {model_name}\")\n",
    "        self.processor = ColPaliProcessor.from_pretrained(model_name)\n",
    "        self.model = ColPali.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.device = self.model.device\n",
    "        self.model.eval()\n",
    "        print(f\"Model loaded on device: {self.device}\")\n",
    "        \n",
    "        self.doc_embeddings = {}\n",
    "        self.page_embeddings = {}\n",
    "    \n",
    "    def encode_queries(self, queries: List[str], batch_size: int = 8) -> List[torch.Tensor]:\n",
    "        all_embeddings = []\n",
    "        \n",
    "        print(f\"Encoding {len(queries)} queries...\")\n",
    "        for i in tqdm(range(0, len(queries), batch_size)):\n",
    "            batch = queries[i:i+batch_size]\n",
    "            inputs = self.processor.process_queries(batch).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embeddings = self.model(**inputs)\n",
    "            \n",
    "            all_embeddings.extend(list(embeddings))\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    def encode_images(self, images: List[Image.Image], batch_size: int = 4) -> List[torch.Tensor]:\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch = images[i:i+batch_size]\n",
    "            inputs = self.processor.process_images(batch).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embeddings = self.model(**inputs)\n",
    "            \n",
    "            all_embeddings.extend(list(embeddings))\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    def late_interaction_score(self, query_emb: torch.Tensor, doc_emb: torch.Tensor) -> float:\n",
    "        similarity_matrix = torch.matmul(query_emb, doc_emb.T)\n",
    "        max_scores = torch.max(similarity_matrix, dim=1)[0]\n",
    "        return torch.sum(max_scores).item()\n",
    "    \n",
    "    def index_documents(self, arxiv_ids: List[str], doc_pages: Dict[str, List[Image.Image]], \n",
    "                       batch_size: int = 4, aggregation: str = \"max\"):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STAGE 1: Indexing Documents\")\n",
    "        print(f\"Aggregation strategy: {aggregation}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for arxiv_id in tqdm(arxiv_ids, desc=\"Encoding documents\"):\n",
    "            pages = doc_pages[arxiv_id]\n",
    "            \n",
    "            if len(pages) == 0:\n",
    "                continue\n",
    "            \n",
    "            page_embeddings = self.encode_images(pages, batch_size=batch_size)\n",
    "            \n",
    "            self.page_embeddings[arxiv_id] = page_embeddings\n",
    "            \n",
    "            stacked = torch.stack(page_embeddings)\n",
    "            \n",
    "            if aggregation == \"mean\":\n",
    "                doc_embedding = torch.mean(stacked, dim=0)\n",
    "            elif aggregation == \"max\":\n",
    "                doc_embedding = torch.max(stacked, dim=0)[0]\n",
    "            elif aggregation == \"weighted\":\n",
    "                weights = torch.softmax(torch.linspace(1.0, 0.5, len(pages)), dim=0)\n",
    "                weights = weights.view(-1, 1, 1).to(stacked.device)\n",
    "                doc_embedding = torch.sum(stacked * weights, dim=0)\n",
    "            else:\n",
    "                doc_embedding = torch.mean(stacked, dim=0)\n",
    "            \n",
    "            self.doc_embeddings[arxiv_id] = doc_embedding\n",
    "        \n",
    "        print(f\"✓ Indexed {len(self.doc_embeddings)} documents\")\n",
    "    \n",
    "    def retrieve_document(self, query_embedding: torch.Tensor, top_k: int = 5) -> List[str]:\n",
    "        scores = {}\n",
    "        \n",
    "        for arxiv_id, doc_emb in self.doc_embeddings.items():\n",
    "            score = self.late_interaction_score(query_embedding, doc_emb)\n",
    "            scores[arxiv_id] = score\n",
    "        \n",
    "        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [arxiv_id for arxiv_id, _ in sorted_docs[:top_k]]\n",
    "    \n",
    "    def retrieve_page_from_document(self, query_embedding: torch.Tensor, \n",
    "                                    arxiv_id: str, top_k: int = 5) -> List[int]:\n",
    "        if arxiv_id not in self.page_embeddings:\n",
    "            return []\n",
    "        \n",
    "        page_embeddings = self.page_embeddings[arxiv_id]\n",
    "        scores = []\n",
    "        \n",
    "        for page_emb in page_embeddings:\n",
    "            score = self.late_interaction_score(query_embedding, page_emb)\n",
    "            scores.append(score)\n",
    "        \n",
    "        scores = np.array(scores)\n",
    "        top_pages = np.argsort(scores)[::-1][:top_k]\n",
    "        \n",
    "        return top_pages.tolist()\n",
    "    \n",
    "    def retrieve_two_stage(self, query_embeddings: List[torch.Tensor],\n",
    "                          top_k_docs: int = 5, top_k_pages: int = 5,\n",
    "                          rerank: bool = True) -> List[List[Tuple[str, int]]]:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TWO-STAGE RETRIEVAL\")\n",
    "        print(f\"Stage 1: Retrieving top {top_k_docs} documents\")\n",
    "        print(f\"Stage 2: Retrieving top {top_k_pages} pages\")\n",
    "        print(f\"Re-ranking: {rerank}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for query_emb in tqdm(query_embeddings, desc=\"Retrieving\"):\n",
    "            top_docs = self.retrieve_document(query_emb, top_k=top_k_docs)\n",
    "            \n",
    "            if rerank:\n",
    "                all_candidates = []\n",
    "                \n",
    "                for arxiv_id in top_docs:\n",
    "                    if arxiv_id not in self.page_embeddings:\n",
    "                        continue\n",
    "                    \n",
    "                    page_embeddings = self.page_embeddings[arxiv_id]\n",
    "                    \n",
    "                    for page_num, page_emb in enumerate(page_embeddings):\n",
    "                        score = self.late_interaction_score(query_emb, page_emb)\n",
    "                        all_candidates.append(((arxiv_id, page_num), score))\n",
    "                \n",
    "                all_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "                top_results = [doc_page for doc_page, _ in all_candidates[:top_k_pages]]\n",
    "                \n",
    "            else:\n",
    "                doc_page_scores = []\n",
    "                \n",
    "                for arxiv_id in top_docs:\n",
    "                    top_pages = self.retrieve_page_from_document(query_emb, arxiv_id, top_k=top_k_pages)\n",
    "                    \n",
    "                    for page_num in top_pages:\n",
    "                        page_emb = self.page_embeddings[arxiv_id][page_num]\n",
    "                        score = self.late_interaction_score(query_emb, page_emb)\n",
    "                        doc_page_scores.append(((arxiv_id, page_num), score))\n",
    "                \n",
    "                doc_page_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                top_results = [dp for dp, _ in doc_page_scores[:top_k_pages]]\n",
    "            \n",
    "            all_results.append(top_results)\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "\n",
    "def evaluate_two_stage(results: List[List[Tuple[str, int]]], \n",
    "                      ground_truth: List[Tuple[str, int]]) -> Dict[str, float]:\n",
    "    \n",
    "    ndcg_5_scores = []\n",
    "    recall_1_scores = []\n",
    "    recall_5_scores = []\n",
    "    mrr_scores = []\n",
    "    \n",
    "    doc_recall_scores = []\n",
    "    page_given_doc_recall = []\n",
    "    end_to_end_success = []\n",
    "    \n",
    "    for result_list, (gt_arxiv_id, gt_page_num) in zip(results, ground_truth):\n",
    "        if gt_arxiv_id is None:\n",
    "            continue\n",
    "        \n",
    "        retrieved_ids = [f\"{arxiv_id}_page_{page_num}\" for arxiv_id, page_num in result_list]\n",
    "        gt_id = f\"{gt_arxiv_id}_page_{gt_page_num}\"\n",
    "        \n",
    "        relevance = np.array([1 if rid == gt_id else 0 for rid in retrieved_ids])\n",
    "        \n",
    "        ndcg_5_scores.append(compute_ndcg_at_k(relevance, k=5))\n",
    "        recall_1_scores.append(1.0 if gt_id in retrieved_ids[:1] else 0.0)\n",
    "        recall_5_scores.append(1.0 if gt_id in retrieved_ids[:5] else 0.0)\n",
    "        \n",
    "        try:\n",
    "            rank = retrieved_ids.index(gt_id) + 1\n",
    "            mrr_scores.append(1.0 / rank)\n",
    "        except ValueError:\n",
    "            mrr_scores.append(0.0)\n",
    "        \n",
    "        retrieved_docs = [arxiv_id for arxiv_id, _ in result_list]\n",
    "        doc_correct = 1.0 if gt_arxiv_id in retrieved_docs else 0.0\n",
    "        doc_recall_scores.append(doc_correct)\n",
    "        \n",
    "        page_correct_given_doc = 0.0\n",
    "        if doc_correct:\n",
    "            pages_from_correct_doc = [page for arxiv, page in result_list if arxiv == gt_arxiv_id]\n",
    "            page_correct_given_doc = 1.0 if gt_page_num in pages_from_correct_doc else 0.0\n",
    "            page_given_doc_recall.append(page_correct_given_doc)\n",
    "        \n",
    "        end_to_end_success.append(1.0 if (gt_arxiv_id, gt_page_num) in result_list else 0.0)\n",
    "    \n",
    "    return {\n",
    "        \"NDCG@5\": np.mean(ndcg_5_scores) * 100,\n",
    "        \"Recall@1\": np.mean(recall_1_scores) * 100,\n",
    "        \"Recall@5\": np.mean(recall_5_scores) * 100,\n",
    "        \"MRR\": np.mean(mrr_scores),\n",
    "        \"Doc_Recall@5\": np.mean(doc_recall_scores) * 100,\n",
    "        \"Page_Given_Doc_Recall\": np.mean(page_given_doc_recall) * 100 if page_given_doc_recall else 0.0,\n",
    "        \"End_to_End_Success\": np.mean(end_to_end_success) * 100,\n",
    "        \"Stage1_Stage2_Product\": np.mean(doc_recall_scores) * np.mean(page_given_doc_recall) if page_given_doc_recall else 0.0\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TWO-STAGE RAG SYSTEM - Open RAGBench Evaluation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "MAX_QUERIES = 50\n",
    "PDF_DIR = \"./arxiv_pdfs\"\n",
    "\n",
    "TOP_K_DOCS = 20\n",
    "TOP_K_PAGES = 20\n",
    "AGGREGATION = \"max\"\n",
    "RERANK = True\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading Open RAGBench Dataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "loader = OpenRAGBenchLoader()\n",
    "\n",
    "loader.download_dataset(max_queries=MAX_QUERIES)\n",
    "loader.load_dataset()\n",
    "\n",
    "pdf_processor = PDFProcessor(pdf_dir=PDF_DIR)\n",
    "\n",
    "colpali_dataset = OpenRAGBenchDataset(loader, pdf_processor, max_queries=MAX_QUERIES)\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Queries: {len(colpali_dataset.queries)}\")\n",
    "print(f\"  Documents: {len(colpali_dataset.arxiv_ids)}\")\n",
    "total_pages = sum(len(pages) for pages in colpali_dataset.doc_to_pages.values())\n",
    "print(f\"  Total pages: {total_pages}\")\n",
    "print(f\"  Avg pages/doc: {total_pages/len(colpali_dataset.arxiv_ids):.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Initializing ColPali RAG System\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "colpali_rag = TwoStageColPaliRAG(\"vidore/colpali-v1.2\")\n",
    "\n",
    "colpali_rag.index_documents(colpali_dataset.arxiv_ids, colpali_dataset.doc_to_pages, \n",
    "                   batch_size=4, aggregation=AGGREGATION)\n",
    "\n",
    "query_embeddings = colpali_rag.encode_queries(colpali_dataset.queries, batch_size=8)\n",
    "\n",
    "results = colpali_rag.retrieve_two_stage(query_embeddings, \n",
    "                                 top_k_docs=TOP_K_DOCS, \n",
    "                                 top_k_pages=TOP_K_PAGES,\n",
    "                                 rerank=RERANK)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "metrics = evaluate_two_stage(results, colpali_dataset.ground_truth)\n",
    "\n",
    "print(\"\\n Standard Retrieval Metrics:\")\n",
    "print(f\"  NDCG@5:   {metrics['NDCG@5']:.2f}%\")\n",
    "print(f\"  Recall@1: {metrics['Recall@1']:.2f}%\")\n",
    "print(f\"  Recall@5: {metrics['Recall@5']:.2f}%\")\n",
    "print(f\"  MRR:      {metrics['MRR']:.4f}\")\n",
    "\n",
    "print(\"\\n Two-Stage Breakdown:\")\n",
    "print(f\"  Stage 1 - Document Recall@5:       {metrics['Doc_Recall@5']:.2f}%\")\n",
    "print(f\"  Stage 2 - Page Recall (given doc): {metrics['Page_Given_Doc_Recall']:.2f}%\")\n",
    "print(f\"  End-to-End Success (exact match):  {metrics['End_to_End_Success']:.2f}%\")\n",
    "print(f\"  Expected compound (Stage1 × Stage2): {metrics['Stage1_Stage2_Product']:.2f}%\")\n",
    "\n",
    "print(\"\\n Current Configuration:\")\n",
    "print(f\"  • top_k_docs: {TOP_K_DOCS}\")\n",
    "print(f\"  • top_k_pages: {TOP_K_PAGES}\")\n",
    "print(f\"  • aggregation: {AGGREGATION}\")\n",
    "print(f\"  • rerank: {RERANK}\")\n",
    "\n",
    "print(\"\\n Tips to Improve Accuracy:\")\n",
    "print(\"  1. Increase TOP_K_DOCS (10→20) - casts wider net in Stage 1\")\n",
    "print(\"  2. Use aggregation='max' - captures most salient content\")\n",
    "print(\"  3. Enable rerank=True - global re-ranking of all candidates\")\n",
    "print(\"  4. Increase TOP_K_PAGES for evaluation metrics\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798d2325-700d-4e69-86e2-24541cb346cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Text-Based RAG Baseline for Open RAGBench\n",
    "Uses OCR + text embeddings (traditional approach)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from PIL import Image\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import json\n",
    "from pathlib import Path\n",
    "import fitz\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "fitz.TOOLS.mupdf_display_errors(False)\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "\n",
    "def compute_ndcg_at_k(relevance_scores: np.ndarray, k: int = 5) -> float:\n",
    "    if len(relevance_scores) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    k = min(k, len(relevance_scores))\n",
    "    top_k_scores = relevance_scores[:k]\n",
    "    \n",
    "    dcg = top_k_scores[0] + np.sum(top_k_scores[1:] / np.log2(np.arange(2, k + 1)))\n",
    "    \n",
    "    ideal_scores = np.sort(relevance_scores)[::-1][:k]\n",
    "    idcg = ideal_scores[0] + np.sum(ideal_scores[1:] / np.log2(np.arange(2, k + 1)))\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def compute_recall_at_k(ranked_indices: List[int], relevant_idx: int, k: int = 1) -> float:\n",
    "    return 1.0 if relevant_idx in ranked_indices[:k] else 0.0\n",
    "\n",
    "def compute_mrr(ranked_indices: List[int], relevant_idx: int) -> float:\n",
    "    try:\n",
    "        rank = ranked_indices.index(relevant_idx) + 1\n",
    "        return 1.0 / rank\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "class PDFTextExtractor:\n",
    "    \n",
    "    def __init__(self, pdf_dir: str = \"./arxiv_pdfs\"):\n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.text_cache = {}\n",
    "    \n",
    "    def extract_text_from_pdf(self, arxiv_id: str) -> List[str]:\n",
    "        if arxiv_id in self.text_cache:\n",
    "            return self.text_cache[arxiv_id]\n",
    "        \n",
    "        pdf_path = self.pdf_dir / f\"{arxiv_id}.pdf\"\n",
    "        \n",
    "        if not pdf_path.exists():\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            page_texts = []\n",
    "            \n",
    "            for page_num in range(len(doc)):\n",
    "                try:\n",
    "                    page = doc[page_num]\n",
    "                    text = page.get_text()\n",
    "                    \n",
    "                    text = text.strip()\n",
    "                    if len(text) < 50:\n",
    "                        text = \"[Page contains minimal text or is image-based]\"\n",
    "                    \n",
    "                    page_texts.append(text)\n",
    "                \n",
    "                except Exception as page_error:\n",
    "                    page_texts.append(\"[Error extracting text from this page]\")\n",
    "                    continue\n",
    "            \n",
    "            doc.close()\n",
    "            \n",
    "            if len(page_texts) > 0:\n",
    "                self.text_cache[arxiv_id] = page_texts\n",
    "            \n",
    "            return page_texts\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  Error extracting text from {arxiv_id}: {str(e)[:100]}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "class TextBasedDataset:\n",
    "    \n",
    "    def __init__(self, loader: OpenRAGBenchLoader, text_extractor: PDFTextExtractor, max_queries: int = None):\n",
    "        self.loader = loader\n",
    "        self.text_extractor = text_extractor\n",
    "        self.max_queries = max_queries\n",
    "        \n",
    "        self._organize_data()\n",
    "    \n",
    "    def _organize_data(self):\n",
    "        print(\"Organizing dataset with text extraction...\")\n",
    "        \n",
    "        self.queries = []\n",
    "        self.query_ids = []\n",
    "        self.ground_truth = []\n",
    "        \n",
    "        all_query_ids = list(self.loader.queries.keys())\n",
    "        \n",
    "        if self.max_queries is not None:\n",
    "            all_query_ids = all_query_ids[:self.max_queries]\n",
    "            print(f\"Limiting to first {self.max_queries} queries\")\n",
    "        \n",
    "        for query_id in all_query_ids:\n",
    "            query_data = self.loader.queries[query_id]\n",
    "            self.query_ids.append(query_id)\n",
    "            self.queries.append(query_data['query'])\n",
    "            \n",
    "            gt_pages = self.loader.get_ground_truth_pages(query_id)\n",
    "            if gt_pages:\n",
    "                self.ground_truth.append(gt_pages[0])\n",
    "            else:\n",
    "                self.ground_truth.append((None, None))\n",
    "        \n",
    "        print(f\"Processing {len(self.queries)} queries\")\n",
    "        \n",
    "        arxiv_ids = set()\n",
    "        for gt in self.ground_truth:\n",
    "            if gt[0] is not None:\n",
    "                arxiv_ids.add(gt[0])\n",
    "        \n",
    "        available_pdfs = set()\n",
    "        pdf_dir = Path(self.text_extractor.pdf_dir)\n",
    "        for pdf_file in pdf_dir.glob(\"*.pdf\"):\n",
    "            arxiv_id = pdf_file.stem\n",
    "            available_pdfs.add(arxiv_id)\n",
    "        \n",
    "        print(f\"Found {len(available_pdfs)} available PDFs\")\n",
    "        arxiv_ids = arxiv_ids.intersection(available_pdfs)\n",
    "        self.arxiv_ids = sorted(list(arxiv_ids))\n",
    "        \n",
    "        filtered_queries = []\n",
    "        filtered_query_ids = []\n",
    "        filtered_ground_truth = []\n",
    "        \n",
    "        for qid, query, gt in zip(self.query_ids, self.queries, self.ground_truth):\n",
    "            if gt[0] in arxiv_ids:\n",
    "                filtered_queries.append(query)\n",
    "                filtered_query_ids.append(qid)\n",
    "                filtered_ground_truth.append(gt)\n",
    "        \n",
    "        self.queries = filtered_queries\n",
    "        self.query_ids = filtered_query_ids\n",
    "        self.ground_truth = filtered_ground_truth\n",
    "        \n",
    "        print(f\"✓ Filtered to {len(self.queries)} queries with available documents\")\n",
    "        \n",
    "        print(f\"Extracting text from {len(self.arxiv_ids)} documents...\")\n",
    "        self.doc_to_page_texts = {}\n",
    "        failed_docs = []\n",
    "        \n",
    "        for arxiv_id in tqdm(self.arxiv_ids, desc=\"Extracting text from PDFs\"):\n",
    "            page_texts = self.text_extractor.extract_text_from_pdf(arxiv_id)\n",
    "            if len(page_texts) > 0:\n",
    "                self.doc_to_page_texts[arxiv_id] = page_texts\n",
    "            else:\n",
    "                failed_docs.append(arxiv_id)\n",
    "        \n",
    "        if failed_docs:\n",
    "            print(f\"⚠ Warning: {len(failed_docs)} PDFs failed text extraction\")\n",
    "            filtered_queries = []\n",
    "            filtered_query_ids = []\n",
    "            filtered_ground_truth = []\n",
    "            \n",
    "            for qid, query, gt in zip(self.query_ids, self.queries, self.ground_truth):\n",
    "                if gt[0] not in failed_docs:\n",
    "                    filtered_queries.append(query)\n",
    "                    filtered_query_ids.append(qid)\n",
    "                    filtered_ground_truth.append(gt)\n",
    "            \n",
    "            self.queries = filtered_queries\n",
    "            self.query_ids = filtered_query_ids\n",
    "            self.ground_truth = filtered_ground_truth\n",
    "        \n",
    "        total_pages = sum(len(texts) for texts in self.doc_to_page_texts.values())\n",
    "        print(f\"✓ Successfully loaded {len(self.queries)} queries\")\n",
    "        print(f\"✓ Successfully extracted text from {len(self.doc_to_page_texts)} documents\")\n",
    "        print(f\"✓ Total pages: {total_pages}\")\n",
    "        avg_pages = total_pages / len(self.doc_to_page_texts) if self.doc_to_page_texts else 0\n",
    "        print(f\"✓ Average pages per document: {avg_pages:.1f}\")\n",
    "\n",
    "\n",
    "class TextEmbeddingModel:\n",
    "    \n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        print(f\"Loading text embedding model: {model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        print(f\"Model loaded on device: {self.device}\")\n",
    "    \n",
    "    def encode_texts(self, texts: List[str], batch_size: int = 32, max_length: int = 512) -> np.ndarray:\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "                all_embeddings.append(embeddings.cpu().numpy())\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    def cosine_similarity(self, query_embeds: np.ndarray, doc_embeds: np.ndarray) -> np.ndarray:\n",
    "        query_norm = query_embeds / (np.linalg.norm(query_embeds, axis=1, keepdims=True) + 1e-9)\n",
    "        doc_norm = doc_embeds / (np.linalg.norm(doc_embeds, axis=1, keepdims=True) + 1e-9)\n",
    "        return query_norm @ doc_norm.T\n",
    "\n",
    "\n",
    "class TextBasedRAG:\n",
    "    \n",
    "    def __init__(self, embedding_model: TextEmbeddingModel):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.doc_embeddings = {}\n",
    "        self.page_embeddings = {}\n",
    "        self.arxiv_ids = []\n",
    "    \n",
    "    def index_documents(self, arxiv_ids: List[str], doc_page_texts: Dict[str, List[str]],\n",
    "                       batch_size: int = 32):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"INDEXING DOCUMENTS (Text-Based)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        self.arxiv_ids = arxiv_ids\n",
    "        \n",
    "        for arxiv_id in tqdm(arxiv_ids, desc=\"Encoding documents\"):\n",
    "            page_texts = doc_page_texts[arxiv_id]\n",
    "            \n",
    "            if len(page_texts) == 0:\n",
    "                continue\n",
    "            \n",
    "            page_embeds = self.embedding_model.encode_texts(page_texts, batch_size=batch_size)\n",
    "            self.page_embeddings[arxiv_id] = page_embeds\n",
    "            \n",
    "            doc_embed = np.mean(page_embeds, axis=0, keepdims=True)\n",
    "            self.doc_embeddings[arxiv_id] = doc_embed\n",
    "        \n",
    "        print(f\"✓ Indexed {len(self.doc_embeddings)} documents\")\n",
    "    \n",
    "    def retrieve_document(self, query_embed: np.ndarray, top_k: int = 5) -> List[str]:\n",
    "        scores = {}\n",
    "        \n",
    "        for arxiv_id, doc_embed in self.doc_embeddings.items():\n",
    "            similarity = self.embedding_model.cosine_similarity(query_embed, doc_embed)\n",
    "            scores[arxiv_id] = similarity[0, 0]\n",
    "        \n",
    "        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [arxiv_id for arxiv_id, _ in sorted_docs[:top_k]]\n",
    "    \n",
    "    def retrieve_page_from_document(self, query_embed: np.ndarray, arxiv_id: str,\n",
    "                                    top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "        if arxiv_id not in self.page_embeddings:\n",
    "            return []\n",
    "        \n",
    "        page_embeds = self.page_embeddings[arxiv_id]\n",
    "        similarities = self.embedding_model.cosine_similarity(query_embed, page_embeds)\n",
    "        \n",
    "        scores = similarities[0]\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        \n",
    "        return [(idx, scores[idx]) for idx in top_indices]\n",
    "    \n",
    "    def retrieve_two_stage(self, queries: List[str], top_k_docs: int = 10,\n",
    "                          top_k_pages: int = 10, batch_size: int = 32) -> List[List[Tuple[str, int]]]:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TWO-STAGE RETRIEVAL (Text-Based)\")\n",
    "        print(f\"Stage 1: Retrieving top {top_k_docs} documents\")\n",
    "        print(f\"Stage 2: Retrieving top {top_k_pages} pages\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(\"Encoding queries...\")\n",
    "        query_embeds = self.embedding_model.encode_texts(queries, batch_size=batch_size)\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for i, query_embed in enumerate(tqdm(query_embeds, desc=\"Retrieving\")):\n",
    "            query_embed = query_embed.reshape(1, -1)\n",
    "            \n",
    "            top_docs = self.retrieve_document(query_embed, top_k=top_k_docs)\n",
    "            \n",
    "            all_candidates = []\n",
    "            \n",
    "            for arxiv_id in top_docs:\n",
    "                page_results = self.retrieve_page_from_document(query_embed, arxiv_id, top_k=top_k_pages)\n",
    "                \n",
    "                for page_num, score in page_results:\n",
    "                    all_candidates.append(((arxiv_id, page_num), score))\n",
    "            \n",
    "            all_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_results = [doc_page for doc_page, _ in all_candidates[:top_k_pages]]\n",
    "            \n",
    "            all_results.append(top_results)\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "\n",
    "def evaluate_two_stage(results: List[List[Tuple[str, int]]], \n",
    "                      ground_truth: List[Tuple[str, int]]) -> Dict[str, float]:\n",
    "    \n",
    "    ndcg_5_scores = []\n",
    "    recall_1_scores = []\n",
    "    recall_5_scores = []\n",
    "    mrr_scores = []\n",
    "    \n",
    "    doc_recall_scores = []\n",
    "    page_given_doc_recall = []\n",
    "    end_to_end_success = []\n",
    "    \n",
    "    for result_list, (gt_arxiv_id, gt_page_num) in zip(results, ground_truth):\n",
    "        if gt_arxiv_id is None:\n",
    "            continue\n",
    "        \n",
    "        retrieved_ids = [f\"{arxiv_id}_page_{page_num}\" for arxiv_id, page_num in result_list]\n",
    "        gt_id = f\"{gt_arxiv_id}_page_{gt_page_num}\"\n",
    "        \n",
    "        relevance = np.array([1 if rid == gt_id else 0 for rid in retrieved_ids])\n",
    "        \n",
    "        ndcg_5_scores.append(compute_ndcg_at_k(relevance, k=5))\n",
    "        recall_1_scores.append(1.0 if gt_id in retrieved_ids[:1] else 0.0)\n",
    "        recall_5_scores.append(1.0 if gt_id in retrieved_ids[:5] else 0.0)\n",
    "        \n",
    "        try:\n",
    "            rank = retrieved_ids.index(gt_id) + 1\n",
    "            mrr_scores.append(1.0 / rank)\n",
    "        except ValueError:\n",
    "            mrr_scores.append(0.0)\n",
    "        \n",
    "        retrieved_docs = [arxiv_id for arxiv_id, _ in result_list]\n",
    "        doc_correct = 1.0 if gt_arxiv_id in retrieved_docs else 0.0\n",
    "        doc_recall_scores.append(doc_correct)\n",
    "        \n",
    "        if doc_correct:\n",
    "            pages_from_correct_doc = [page for arxiv, page in result_list if arxiv == gt_arxiv_id]\n",
    "            page_correct_given_doc = 1.0 if gt_page_num in pages_from_correct_doc else 0.0\n",
    "            page_given_doc_recall.append(page_correct_given_doc)\n",
    "        \n",
    "        end_to_end_success.append(1.0 if (gt_arxiv_id, gt_page_num) in result_list else 0.0)\n",
    "    \n",
    "    return {\n",
    "        \"NDCG@5\": np.mean(ndcg_5_scores) * 100,\n",
    "        \"Recall@1\": np.mean(recall_1_scores) * 100,\n",
    "        \"Recall@5\": np.mean(recall_5_scores) * 100,\n",
    "        \"MRR\": np.mean(mrr_scores),\n",
    "        \"Doc_Recall@5\": np.mean(doc_recall_scores) * 100,\n",
    "        \"Page_Given_Doc_Recall\": np.mean(page_given_doc_recall) * 100 if page_given_doc_recall else 0.0,\n",
    "        \"End_to_End_Success\": np.mean(end_to_end_success) * 100,\n",
    "        \"Stage1_Stage2_Product\": np.mean(doc_recall_scores) * np.mean(page_given_doc_recall) if page_given_doc_recall else 0.0\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEXT-BASED RAG BASELINE - Open RAGBench Evaluation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "MAX_QUERIES = 50\n",
    "PDF_DIR = \"./arxiv_pdfs\"\n",
    "TOP_K_DOCS = 20\n",
    "TOP_K_PAGES = 20\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading Open RAGBench Dataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "loader = OpenRAGBenchLoader()\n",
    "loader.download_dataset(max_queries=MAX_QUERIES)\n",
    "loader.load_dataset()\n",
    "\n",
    "text_extractor = PDFTextExtractor(pdf_dir=PDF_DIR)\n",
    "\n",
    "text_dataset = TextBasedDataset(loader, text_extractor, max_queries=MAX_QUERIES)\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Queries: {len(text_dataset.queries)}\")\n",
    "print(f\"  Documents: {len(text_dataset.arxiv_ids)}\")\n",
    "total_pages = sum(len(texts) for texts in text_dataset.doc_to_page_texts.values())\n",
    "print(f\"  Total pages: {total_pages}\")\n",
    "print(f\"  Avg pages/doc: {total_pages/len(text_dataset.arxiv_ids):.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Initializing Text-Based RAG System\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "embedding_model = TextEmbeddingModel(model_name=EMBEDDING_MODEL)\n",
    "text_rag = TextBasedRAG(embedding_model)\n",
    "\n",
    "text_rag.index_documents(text_dataset.arxiv_ids, text_dataset.doc_to_page_texts, batch_size=32)\n",
    "\n",
    "results = text_rag.retrieve_two_stage(text_dataset.queries, \n",
    "                                 top_k_docs=TOP_K_DOCS,\n",
    "                                 top_k_pages=TOP_K_PAGES,\n",
    "                                 batch_size=32)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION RESULTS (Text-Based Baseline)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "metrics = evaluate_two_stage(results, text_dataset.ground_truth)\n",
    "\n",
    "print(\"\\nStandard Retrieval Metrics:\")\n",
    "print(f\"  NDCG@5:   {metrics['NDCG@5']:.2f}%\")\n",
    "print(f\"  Recall@1: {metrics['Recall@1']:.2f}%\")\n",
    "print(f\"  Recall@5: {metrics['Recall@5']:.2f}%\")\n",
    "print(f\"  MRR:      {metrics['MRR']:.4f}\")\n",
    "\n",
    "print(\"\\nTwo-Stage Breakdown:\")\n",
    "print(f\"  Stage 1 - Document Recall@5:       {metrics['Doc_Recall@5']:.2f}%\")\n",
    "print(f\"  Stage 2 - Page Recall (given doc): {metrics['Page_Given_Doc_Recall']:.2f}%\")\n",
    "print(f\"  End-to-End Success (exact match):  {metrics['End_to_End_Success']:.2f}%\")\n",
    "\n",
    "print(\"\\n Analysis:\")\n",
    "print(f\"  • Text-based RAG (OCR + embeddings)\")\n",
    "print(f\"  • Stage 1: {metrics['Doc_Recall@5']:.0f}% document recall\")\n",
    "print(f\"  • Stage 2: {metrics['Page_Given_Doc_Recall']:.0f}% page recall (given doc)\")\n",
    "print(f\"  • Compound: {metrics['Stage1_Stage2_Product']:.0f}%\")\n",
    "\n",
    "print(\"\\nLimitations of Text-Based Approach:\")\n",
    "print(\"  ⚠ Loses visual information (figures, tables, layouts)\")\n",
    "print(\"  ⚠ OCR errors on complex PDFs\")\n",
    "print(\"  ⚠ Long text truncation (512 tokens max)\")\n",
    "print(\"  ⚠ No understanding of visual structure\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4b1a59-71a5-4f03-95a6-3119d1fef719",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Latency Benchmark for RAG Systems\n",
    "Measures P95 and P99 latencies for ColPali and Text-Based RAG\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "@dataclass\n",
    "class LatencyMetrics:\n",
    "    operation: str\n",
    "    mean_ms: float\n",
    "    median_ms: float\n",
    "    p50_ms: float\n",
    "    p95_ms: float\n",
    "    p99_ms: float\n",
    "    min_ms: float\n",
    "    max_ms: float\n",
    "    std_ms: float\n",
    "    num_samples: int\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "class LatencyProfiler:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.measurements = {}\n",
    "    \n",
    "    def measure(self, operation_name: str):\n",
    "        return LatencyContext(self, operation_name)\n",
    "    \n",
    "    def add_measurement(self, operation: str, latency_ms: float):\n",
    "        if operation not in self.measurements:\n",
    "            self.measurements[operation] = []\n",
    "        self.measurements[operation].append(latency_ms)\n",
    "    \n",
    "    def compute_metrics(self, operation: str) -> LatencyMetrics:\n",
    "        if operation not in self.measurements:\n",
    "            return None\n",
    "        \n",
    "        latencies = np.array(self.measurements[operation])\n",
    "        \n",
    "        return LatencyMetrics(\n",
    "            operation=operation,\n",
    "            mean_ms=float(np.mean(latencies)),\n",
    "            median_ms=float(np.median(latencies)),\n",
    "            p50_ms=float(np.percentile(latencies, 50)),\n",
    "            p95_ms=float(np.percentile(latencies, 95)),\n",
    "            p99_ms=float(np.percentile(latencies, 99)),\n",
    "            min_ms=float(np.min(latencies)),\n",
    "            max_ms=float(np.max(latencies)),\n",
    "            std_ms=float(np.std(latencies)),\n",
    "            num_samples=len(latencies)\n",
    "        )\n",
    "    \n",
    "    def get_all_metrics(self) -> Dict[str, LatencyMetrics]:\n",
    "        return {op: self.compute_metrics(op) for op in self.measurements.keys()}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.measurements = {}\n",
    "\n",
    "class LatencyContext:\n",
    "    \n",
    "    def __init__(self, profiler: LatencyProfiler, operation: str):\n",
    "        self.profiler = profiler\n",
    "        self.operation = operation\n",
    "        self.start_time = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start_time = time.perf_counter()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        end_time = time.perf_counter()\n",
    "        latency_ms = (end_time - self.start_time) * 1000\n",
    "        self.profiler.add_measurement(self.operation, latency_ms)\n",
    "\n",
    "\n",
    "class ColPaliLatencyBenchmark:\n",
    "    \n",
    "    def __init__(self, rag_system, dataset, profiler: LatencyProfiler):\n",
    "        self.rag = rag_system\n",
    "        self.dataset = dataset\n",
    "        self.profiler = profiler\n",
    "    \n",
    "    def benchmark_query_encoding(self, num_samples: int = 100):\n",
    "        print(\"\\n📊 Benchmarking ColPali Query Encoding...\")\n",
    "        \n",
    "        sample_queries = self.dataset.queries[:num_samples]\n",
    "        \n",
    "        _ = self.rag.encode_queries(sample_queries[:5], batch_size=1)\n",
    "        \n",
    "        for query in tqdm(sample_queries, desc=\"  Single query encoding\"):\n",
    "            with self.profiler.measure(\"colpali_query_encode_single\"):\n",
    "                _ = self.rag.encode_queries([query], batch_size=1)\n",
    "        \n",
    "        for i in range(0, len(sample_queries), 8):\n",
    "            batch = sample_queries[i:i+8]\n",
    "            with self.profiler.measure(\"colpali_query_encode_batch8\"):\n",
    "                _ = self.rag.encode_queries(batch, batch_size=8)\n",
    "    \n",
    "    def benchmark_document_encoding(self, num_docs: int = 20):\n",
    "        print(\"\\n📊 Benchmarking ColPali Document Encoding...\")\n",
    "        \n",
    "        sample_docs = list(self.dataset.doc_to_pages.items())[:num_docs]\n",
    "        \n",
    "        for arxiv_id, pages in tqdm(sample_docs, desc=\"  Document encoding\"):\n",
    "            if len(pages) > 0:\n",
    "                with self.profiler.measure(\"colpali_page_encode_single\"):\n",
    "                    _ = self.rag.encode_images([pages[0]], batch_size=1)\n",
    "            \n",
    "            for i in range(0, min(len(pages), 12), 4):\n",
    "                batch = pages[i:i+4]\n",
    "                with self.profiler.measure(\"colpali_page_encode_batch4\"):\n",
    "                    _ = self.rag.encode_images(batch, batch_size=4)\n",
    "    \n",
    "    def benchmark_stage1_retrieval(self, num_queries: int = 100):\n",
    "        print(\"\\n📊 Benchmarking ColPali Stage 1 (Document Retrieval)...\")\n",
    "        \n",
    "        sample_queries = self.dataset.queries[:num_queries]\n",
    "        query_embeddings = self.rag.encode_queries(sample_queries, batch_size=8)\n",
    "        \n",
    "        for query_emb in tqdm(query_embeddings, desc=\"  Stage 1 retrieval\"):\n",
    "            with self.profiler.measure(\"colpali_stage1_retrieve_top5\"):\n",
    "                _ = self.rag.retrieve_document(query_emb, top_k=5)\n",
    "            \n",
    "            with self.profiler.measure(\"colpali_stage1_retrieve_top10\"):\n",
    "                _ = self.rag.retrieve_document(query_emb, top_k=10)\n",
    "    \n",
    "    def benchmark_stage2_retrieval(self, num_queries: int = 100):\n",
    "        print(\"\\n📊 Benchmarking ColPali Stage 2 (Page Retrieval)...\")\n",
    "        \n",
    "        sample_queries = self.dataset.queries[:num_queries]\n",
    "        query_embeddings = self.rag.encode_queries(sample_queries, batch_size=8)\n",
    "        \n",
    "        sample_doc = self.dataset.arxiv_ids[0]\n",
    "        \n",
    "        for query_emb in tqdm(query_embeddings, desc=\"  Stage 2 retrieval\"):\n",
    "            with self.profiler.measure(\"colpali_stage2_retrieve_top5\"):\n",
    "                _ = self.rag.retrieve_page_from_document(query_emb, sample_doc, top_k=5)\n",
    "            \n",
    "            with self.profiler.measure(\"colpali_stage2_retrieve_top10\"):\n",
    "                _ = self.rag.retrieve_page_from_document(query_emb, sample_doc, top_k=10)\n",
    "    \n",
    "    def benchmark_end_to_end(self, num_queries: int = 50):\n",
    "        print(\"\\n📊 Benchmarking ColPali End-to-End Retrieval...\")\n",
    "        \n",
    "        sample_queries = self.dataset.queries[:num_queries]\n",
    "        \n",
    "        for query in tqdm(sample_queries, desc=\"  End-to-end retrieval\"):\n",
    "            with self.profiler.measure(\"colpali_e2e_top5_docs_top5_pages\"):\n",
    "                query_emb = self.rag.encode_queries([query], batch_size=1)\n",
    "                _ = self.rag.retrieve_two_stage(query_emb, top_k_docs=5, top_k_pages=5)\n",
    "            \n",
    "            with self.profiler.measure(\"colpali_e2e_top10_docs_top10_pages\"):\n",
    "                query_emb = self.rag.encode_queries([query], batch_size=1)\n",
    "                _ = self.rag.retrieve_two_stage(query_emb, top_k_docs=10, top_k_pages=10)\n",
    "\n",
    "\n",
    "class TextBasedLatencyBenchmark:\n",
    "    \n",
    "    def __init__(self, rag_system, dataset, profiler: LatencyProfiler):\n",
    "        self.rag = rag_system\n",
    "        self.dataset = dataset\n",
    "        self.profiler = profiler\n",
    "    \n",
    "    def benchmark_query_encoding(self, num_samples: int = 100):\n",
    "        print(\"\\n📊 Benchmarking Text-Based Query Encoding...\")\n",
    "        \n",
    "        sample_queries = self.dataset.queries[:num_samples]\n",
    "        \n",
    "        _ = self.rag.embedding_model.encode_texts(sample_queries[:5], batch_size=1)\n",
    "        \n",
    "        for query in tqdm(sample_queries, desc=\"  Single query encoding\"):\n",
    "            with self.profiler.measure(\"text_query_encode_single\"):\n",
    "                _ = self.rag.embedding_model.encode_texts([query], batch_size=1)\n",
    "        \n",
    "        for i in range(0, len(sample_queries), 32):\n",
    "            batch = sample_queries[i:i+32]\n",
    "            with self.profiler.measure(\"text_query_encode_batch32\"):\n",
    "                _ = self.rag.embedding_model.encode_texts(batch, batch_size=32)\n",
    "    \n",
    "    def benchmark_document_encoding(self, num_docs: int = 20):\n",
    "        print(\"\\n📊 Benchmarking Text-Based Document Encoding...\")\n",
    "        \n",
    "        sample_docs = list(self.dataset.doc_to_page_texts.items())[:num_docs]\n",
    "        \n",
    "        for arxiv_id, page_texts in tqdm(sample_docs, desc=\"  Document encoding\"):\n",
    "            if len(page_texts) > 0:\n",
    "                with self.profiler.measure(\"text_page_encode_single\"):\n",
    "                    _ = self.rag.embedding_model.encode_texts([page_texts[0]], batch_size=1)\n",
    "            \n",
    "            if len(page_texts) >= 8:\n",
    "                with self.profiler.measure(\"text_page_encode_batch8\"):\n",
    "                    _ = self.rag.embedding_model.encode_texts(page_texts[:8], batch_size=8)\n",
    "    \n",
    "    def benchmark_stage1_retrieval(self, num_queries: int = 100):\n",
    "        print(\"\\n📊 Benchmarking Text-Based Stage 1 (Document Retrieval)...\")\n",
    "        \n",
    "        sample_queries = self.dataset.queries[:num_queries]\n",
    "        query_embeds = self.rag.embedding_model.encode_texts(sample_queries, batch_size=32)\n",
    "        \n",
    "        for query_emb in tqdm(query_embeds, desc=\"  Stage 1 retrieval\"):\n",
    "            query_emb = query_emb.reshape(1, -1)\n",
    "            \n",
    "            with self.profiler.measure(\"text_stage1_retrieve_top5\"):\n",
    "                _ = self.rag.retrieve_document(query_emb, top_k=5)\n",
    "            \n",
    "            with self.profiler.measure(\"text_stage1_retrieve_top10\"):\n",
    "                _ = self.rag.retrieve_document(query_emb, top_k=10)\n",
    "    \n",
    "    def benchmark_stage2_retrieval(self, num_queries: int = 100):\n",
    "        print(\"\\n📊 Benchmarking Text-Based Stage 2 (Page Retrieval)...\")\n",
    "        \n",
    "        sample_queries = self.dataset.queries[:num_queries]\n",
    "        query_embeds = self.rag.embedding_model.encode_texts(sample_queries, batch_size=32)\n",
    "        \n",
    "        sample_doc = self.dataset.arxiv_ids[0]\n",
    "        \n",
    "        for query_emb in tqdm(query_embeds, desc=\"  Stage 2 retrieval\"):\n",
    "            query_emb = query_emb.reshape(1, -1)\n",
    "            \n",
    "            with self.profiler.measure(\"text_stage2_retrieve_top5\"):\n",
    "                _ = self.rag.retrieve_page_from_document(query_emb, sample_doc, top_k=5)\n",
    "            \n",
    "            with self.profiler.measure(\"text_stage2_retrieve_top10\"):\n",
    "                _ = self.rag.retrieve_page_from_document(query_emb, sample_doc, top_k=10)\n",
    "    \n",
    "    def benchmark_end_to_end(self, num_queries: int = 50):\n",
    "        print(\"\\n📊 Benchmarking Text-Based End-to-End Retrieval...\")\n",
    "        \n",
    "        sample_queries = self.dataset.queries[:num_queries]\n",
    "        \n",
    "        for query in tqdm(sample_queries, desc=\"  End-to-end retrieval\"):\n",
    "            with self.profiler.measure(\"text_e2e_top5_docs_top5_pages\"):\n",
    "                _ = self.rag.retrieve_two_stage([query], top_k_docs=5, top_k_pages=5, batch_size=1)\n",
    "            \n",
    "            with self.profiler.measure(\"text_e2e_top10_docs_top10_pages\"):\n",
    "                _ = self.rag.retrieve_two_stage([query], top_k_docs=10, top_k_pages=10, batch_size=1)\n",
    "\n",
    "\n",
    "def print_latency_report(metrics: Dict[str, LatencyMetrics], title: str):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"{title}\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Operation':<45} {'Mean':<10} {'P50':<10} {'P95':<10} {'P99':<10} {'Samples':<10}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for op_name, metric in sorted(metrics.items()):\n",
    "        print(f\"{metric.operation:<45} \"\n",
    "              f\"{metric.mean_ms:>8.2f}ms \"\n",
    "              f\"{metric.p50_ms:>8.2f}ms \"\n",
    "              f\"{metric.p95_ms:>8.2f}ms \"\n",
    "              f\"{metric.p99_ms:>8.2f}ms \"\n",
    "              f\"{metric.num_samples:>10}\")\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "\n",
    "def compare_systems(colpali_metrics: Dict[str, LatencyMetrics], \n",
    "                   text_metrics: Dict[str, LatencyMetrics]):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"SYSTEM COMPARISON - P95 and P99 Latencies\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    comparison_ops = [\n",
    "        (\"Query Encoding (Single)\", \"colpali_query_encode_single\", \"text_query_encode_single\"),\n",
    "        (\"Stage 1: Document Retrieval (Top-5)\", \"colpali_stage1_retrieve_top5\", \"text_stage1_retrieve_top5\"),\n",
    "        (\"Stage 2: Page Retrieval (Top-5)\", \"colpali_stage2_retrieve_top5\", \"text_stage2_retrieve_top5\"),\n",
    "        (\"End-to-End (Top-5 docs, Top-5 pages)\", \"colpali_e2e_top5_docs_top5_pages\", \"text_e2e_top5_docs_top5_pages\"),\n",
    "        (\"End-to-End (Top-10 docs, Top-10 pages)\", \"colpali_e2e_top10_docs_top10_pages\", \"text_e2e_top10_docs_top10_pages\"),\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n{'Operation':<45} {'System':<15} {'P95 (ms)':<12} {'P99 (ms)':<12} {'Winner'}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for op_name, colpali_key, text_key in comparison_ops:\n",
    "        if colpali_key in colpali_metrics and text_key in text_metrics:\n",
    "            colpali_m = colpali_metrics[colpali_key]\n",
    "            text_m = text_metrics[text_key]\n",
    "            \n",
    "            print(f\"{op_name:<45} {'ColPali':<15} {colpali_m.p95_ms:>10.2f}  {colpali_m.p99_ms:>10.2f}\")\n",
    "            \n",
    "            winner = \"Text-Based ✓\" if text_m.p95_ms < colpali_m.p95_ms else \"ColPali ✓\"\n",
    "            speedup = colpali_m.p95_ms / text_m.p95_ms if text_m.p95_ms > 0 else 1.0\n",
    "            print(f\"{'':<45} {'Text-Based':<15} {text_m.p95_ms:>10.2f}  {text_m.p99_ms:>10.2f}  {winner} ({speedup:.1f}x)\")\n",
    "            print(\"-\"*100)\n",
    "\n",
    "def save_results(colpali_metrics: Dict[str, LatencyMetrics],\n",
    "                text_metrics: Dict[str, LatencyMetrics],\n",
    "                output_file: str = \"latency_benchmark_results.json\"):\n",
    "    results = {\n",
    "        \"colpali\": {k: v.to_dict() for k, v in colpali_metrics.items()},\n",
    "        \"text_based\": {k: v.to_dict() for k, v in text_metrics.items()},\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Results saved to {output_file}\")\n",
    "\n",
    "\n",
    "def run_full_benchmark(colpali_rag, colpali_dataset, text_rag, text_dataset,\n",
    "                      num_queries: int = 100, num_docs: int = 20):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"LATENCY BENCHMARK - ColPali vs Text-Based RAG\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    colpali_profiler = LatencyProfiler()\n",
    "    text_profiler = LatencyProfiler()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"BENCHMARKING COLPALI RAG SYSTEM\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    colpali_bench = ColPaliLatencyBenchmark(colpali_rag, colpali_dataset, colpali_profiler)\n",
    "    colpali_bench.benchmark_query_encoding(num_samples=num_queries)\n",
    "    colpali_bench.benchmark_document_encoding(num_docs=num_docs)\n",
    "    colpali_bench.benchmark_stage1_retrieval(num_queries=num_queries)\n",
    "    colpali_bench.benchmark_stage2_retrieval(num_queries=num_queries)\n",
    "    colpali_bench.benchmark_end_to_end(num_queries=min(50, num_queries))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"BENCHMARKING TEXT-BASED RAG SYSTEM\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    text_bench = TextBasedLatencyBenchmark(text_rag, text_dataset, text_profiler)\n",
    "    text_bench.benchmark_query_encoding(num_samples=num_queries)\n",
    "    text_bench.benchmark_document_encoding(num_docs=num_docs)\n",
    "    text_bench.benchmark_stage1_retrieval(num_queries=num_queries)\n",
    "    text_bench.benchmark_stage2_retrieval(num_queries=num_queries)\n",
    "    text_bench.benchmark_end_to_end(num_queries=min(50, num_queries))\n",
    "    \n",
    "    colpali_metrics = colpali_profiler.get_all_metrics()\n",
    "    text_metrics = text_profiler.get_all_metrics()\n",
    "    \n",
    "    print_latency_report(colpali_metrics, \"COLPALI RAG - LATENCY METRICS\")\n",
    "    print_latency_report(text_metrics, \"TEXT-BASED RAG - LATENCY METRICS\")\n",
    "    compare_systems(colpali_metrics, text_metrics)\n",
    "    \n",
    "    save_results(colpali_metrics, text_metrics)\n",
    "    \n",
    "    return colpali_metrics, text_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd14dcb-46f9-4990-b691-e123af163dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "colpali_metrics, text_metrics = run_full_benchmark(\n",
    "    colpali_rag=colpali_rag,\n",
    "    colpali_dataset=colpali_dataset,\n",
    "    text_rag=text_rag,\n",
    "    text_dataset=text_dataset,\n",
    "    num_queries=100,\n",
    "    num_docs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8123c-f24f-4cb7-9029-53bc5461f23f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
